{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras_CNN_Models_demos",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VictorDu1990/Keras_TensorFlow_DL_demo/blob/master/Keras_CNN_Models_demos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQW5trXbyjWu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 01.基于Keras的LeNet实现。\n",
        "\n",
        "#下面对上面的网络架构进行解释：\n",
        "#1.输入图像是单通道的28 x 28 大小的图像，用矩阵表示[1,28,28]。\n",
        "#2.第一个卷积层conv1，卷积核为5 x 5，通道数为20，步长1，经过卷积后，图像尺寸变为28 - 5 + 1 = 24，用矩阵表示[20,24,24]。\n",
        "#3.第一个池化层Max1，最大化池化，池化核尺寸为2 x 2，步长为2，无重叠池化，经过池化后，图像尺寸减半，变为12，用矩阵表示[20,12,12]。\n",
        "#4.第二个卷积层conv2，卷积核为5 x 5，通道数为50，步长1，经过卷积后，图像尺寸变为12 - 5 + 1 = 8，用矩阵表示[50,8,8]。\n",
        "#5.第二个池化层Max2，最大化池化，池化核尺寸为2 x 2，步长为2，无重叠池化，经过池化后，图像尺寸减半，变为4，用矩阵表示[50,4,4]。\n",
        "#6.第一个全连接层fc1，神经元个数为500个，得到一个500维的向量特征。后面作用一个ReLu的激活函数.\n",
        "#7.第二个全连接层fc2，神经元个数为10个，得到一个10维的向量特征。后面作用一个softmax函数，用于预测手写体数字的10个分类的预测。\n",
        "#原文：https://blog.csdn.net/weixin_41843918/article/details/89708730 \n",
        "\n",
        "def LeNet():\n",
        "\t# 定义模型\n",
        "    model = Sequential()\n",
        "    # conv1\n",
        "    model.add(Conv2D(32,(5,5),strides=(1,1),input_shape=(28,28,1),padding='valid',activation='relu',kernel_initializer='uniform'))\n",
        "    # max1\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    # conv2\n",
        "    model.add(Conv2D(64,(5,5),strides=(1,1),padding='valid',activation='relu',kernel_initializer='uniform'))\n",
        "    # max2\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    # 多通道压平\n",
        "    model.add(Flatten())\n",
        "    # fc1\n",
        "    model.add(Dense(500,activation='relu'))\n",
        "    # fc2\n",
        "    model.add(Dense(10,activation='softmax'))\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbHwJNSNzGup",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#02. 基于keras的ALexNet的实现。\n",
        "#1.conv1层，使用较大的11 x 11窗口来捕获物体。同时使用步幅 4 来较大幅度减小输出高和宽。这里使用的输出通道为48，比LeNet的通道数多很多。\n",
        "#2.max1层，池化核尺寸为3 x 3，步幅为2，减小卷积的窗口\n",
        "#3.conv2层，卷积核尺寸为5 x 5，使用填充为padding = 2来使得输入与输出的高和宽一致，且增大了输出通道数。\n",
        "#4.max2层，池化核尺寸为3 x 3，步幅为2，减小卷积的窗口\n",
        "#5.conv3层，卷积核尺寸为3 x 3，步幅为1，自动填充\n",
        "#6.conv4层，卷积核尺寸为3 x 3，步幅为1，自动填充\n",
        "#7.conv5层，卷积核尺寸为3 x 3，步幅为1，自动填充\n",
        "#8.max3层，池化核尺寸为3 x 3，步幅为2，减小卷积的窗口\n",
        "#9.fc1、fc2层，全连接层的输出个数比LeNet中的大数倍，使用dropout来缓解过拟合\n",
        "#10.fc3输出层，根据需要输出类别个数。\n",
        "\n",
        "\n",
        "def AlexNet():\n",
        "\t# 定义模型\n",
        "    model = Sequential()\n",
        "    # conv1，卷积核11 * 11，步长4，第一层要指定输入的形状\n",
        "    model.add(Conv2D(96,(11,11),strides=(4,4),input_shape=(227,227,3),padding='valid',activation='relu',kernel_initializer='uniform'))\n",
        "    # Max1，池化核3 * 3，步长2\n",
        "    model.add(MaxPooling2D(pool_size=(3,3),strides=(2,2)))\n",
        "    # conv2，卷积核5 * 5，自动padding\n",
        "    model.add(Conv2D(256,(5,5),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))\n",
        "    # Max2，池化核3 * 3 ，步长2\n",
        "    model.add(MaxPooling2D(pool_size=(3,3),strides=(2,2)))\n",
        "    # conv3，卷积核 3 * 3，步长1，连续3个卷积层\n",
        "    model.add(Conv2D(384,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))\n",
        "    model.add(Conv2D(384,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))\n",
        "    model.add(Conv2D(256,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))\n",
        "    # Max3，池化核3 * 3，步长2\n",
        "    model.add(MaxPooling2D(pool_size=(3,3),strides=(2,2)))\n",
        "    # 向量化\n",
        "    model.add(Flatten())\n",
        "    # FC1，全连接，后面紧接一个dropout，降低复杂度\n",
        "    model.add(Dense(4096,activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    # FC2， 全连接\n",
        "    model.add(Dense(4096,activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    # FC3，输出层，多分类softmax作用\n",
        "    model.add(Dense(1000,activation='softmax'))\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9k4GoC48z2qa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#03. 基于keras的VGG_Net的实现。\n",
        "\n",
        "#对VGG-16网络结构简单说明：\n",
        "#上述的网络结构不再逐层分析，大致是前面的卷积部分和后面的全连接部分，而全连接部分也是平移了AlexNet的3层全连接。\n",
        "\n",
        "#VGG的特点：\n",
        "#小卷积核。设计者将卷积核全部替换为3 x 3，少数部分用到了1 x 1。\n",
        "#小池化核。相比于AlexNet的3 x 3的卷积核，全部使用的2 x 2。\n",
        "#层数更深特征图更宽。基于前两点外，由于卷积核专注于扩大通道数、池化专注于缩小宽和高，使得模型架构上更深更宽的同时，计算量的增加放缓；\n",
        "#全连接转卷积。网络测试阶段将训练阶段的三个全连接替换为三个卷积，测试重用训练时的参数，使得测试得到的全卷积网络因为没有全连接的限制，因而可以接收任意宽或高为的输入。\n",
        "\n",
        "#小卷积核的有点：\n",
        "#多个小卷积核比一个大卷积核有更多的非线性。使得判决函数更加具有判决性。\n",
        "#多个小卷积核与一个大的卷积核的计算参数相差无几，但是计算量却是大大增加。\n",
        "#大卷积核的计算量比较大。\n",
        "#1 x 1的卷积核，可以在不影响输入和输出维度的前提下，对输入进行线性变换，然后通过ReLU进行非线性变换，增加网络的非线性表达能力。\n",
        "\n",
        "def VGG_16():   \n",
        "\t# 定义模型\n",
        "    model = Sequential()\n",
        "    \n",
        "    # vgg_block1，2个卷积层，后面接一个池化\n",
        "    model.add(Conv2D(64,(3,3),strides=(1,1),input_shape=(224,224,3),padding='same',activation='relu',kernel_initializer='uniform'))\n",
        "    model.add(Conv2D(64,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    \n",
        "    # vgg_block2，2个卷积层，后面接一个池化\n",
        "    model.add(Conv2D(128,(3,2),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))\n",
        "    model.add(Conv2D(128,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    \n",
        "    # vgg_block3，3个卷积层，后面接一个池化\n",
        "    model.add(Conv2D(256,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))\n",
        "    model.add(Conv2D(256,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))\n",
        "    model.add(Conv2D(256,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    \n",
        "    # vgg_block4，3个卷积层，后面接一个池化\n",
        "    model.add(Conv2D(512,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))\n",
        "    model.add(Conv2D(512,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))\n",
        "    model.add(Conv2D(512,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    \n",
        "    # vgg_block5，3个卷积层，后面接一个池化\n",
        "    model.add(Conv2D(512,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))\n",
        "    model.add(Conv2D(512,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))\n",
        "    model.add(Conv2D(512,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    \n",
        "    model.add(Flatten())\n",
        "\t\n",
        "\t# 2个FC层，隐藏层\n",
        "    model.add(Dense(4096,activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(4096,activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "\t\n",
        "\t# 1个FC层，输出层\n",
        "    model.add(Dense(1000,activation='softmax'))\n",
        "    \n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OvSa4280zxS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#04. 基于keras的NIN 实现。\n",
        "\n",
        "#前面介绍了LeNet、AlexNet、VGG网络结构，三者在设计上的共同之处是：先以由卷积层构成的模块充分抽取空间特征，再以由全连接层构成的模块来输出分类结果（简单说就是：卷积层+全连接层）。\n",
        "#其中，AlexNet和VGG对LeNet的改进主要在于如何对这两个模块加宽（增加通道数）和加深（增加层）。下面介绍的NiN（Network in Network）网络，它提出了另外的一个思路，\n",
        "#即串联多个由卷积层和“全连接”层构成的小网络来构建一个深层网络。\n",
        "\n",
        "#1 x 1卷积的作用：\n",
        "#1.实现跨通道的交互和信息整合。\n",
        "#2.进行卷积核通道数的降维和升维。\n",
        "#说明：在NiN网络中这个应用很多，就是用多层卷积网络（MLP）代替传统卷积层。\n",
        "#全局平均池化\n",
        "#1.使用平均池化代替全连接\n",
        "#2.很大程度上减少参数空间，便于加深网络和训练，有效降低过拟合。\n",
        "\n",
        "# NIN块\n",
        "#我们知道，在卷积层的输入和输出通常是4维的数组（样本，通道，高，宽）；而全连接层的输入和输出则通常是二维数组（样本，特征）。如果想在全连接层后在接上卷积层，\n",
        "#则需要将全联机的输出变换成4维。\n",
        "#在前面介绍“多输入通道和多输出通道”时，提到了1 x 1卷积层。他可以看成全连接层，其中空间维度（高和宽）上的每个元素相当于样本，通道相当于特征。\n",
        "#因此，NiN使用1 x 1 卷积层来替代全连接层，从而使空间信息能够自然传递到后面的层中去。\n",
        "\n",
        "#NiN块是NiN中的基础块。它由一个卷积层加两个充当全连接层的1 x 1卷积层串联而成。其中第一个卷积层的超参数可以自行配置，而第二个和第三个卷积层的超参数一般是固定的。\n",
        "\n",
        "from mxnet import gluon,init,nd\n",
        "from mxnet.gluon import nn\n",
        "def nin_block(num_channels,kernel_size,strides,padding):\n",
        "    \"\"\"\n",
        "    构建一个NiN块，这个块也叫做MLPconv（多层感知卷积层），其实就是：传统卷积层+1 x 1卷积层。\n",
        "    用这个NiN块代替传统卷积可以增强网络提取抽象特征和泛化能力。\n",
        "    \n",
        "    Parameters:\n",
        "    ----------------\n",
        "    num_channels:通道数，也就是卷积后的厚度\n",
        "    kernel_size:卷积核的形状\n",
        "    strides:步幅\n",
        "    padding:填充\n",
        "    \n",
        "    return:\n",
        "    ----------------\n",
        "    blk:定义好的一个mlpconv结构\n",
        "    \"\"\"\n",
        "\n",
        "    blk = nn.Sequential()\n",
        "    blk.add(nn.Conv2D(num_channels,kernel_size,strides,padding,activation = 'relu'),\n",
        "            nn.Conv2D(num_channels,kernel_size = 1,activation = 'relu'),\n",
        "            nn.Conv2D(num_channels,kernel_size = 1,activation = 'relu'))\n",
        "    return blk\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Np54DTuY2Aot",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#NiN是AlexNet问世不久后提出的。他们的卷积层设定有类似之处。NiN使用卷积窗口形状分别为11 x 11、5 x 5和3 x 3 的卷积层，相应的输出通道数也与AlexNet中的一致。\n",
        "#每个NiN块后接一个步幅为2、窗口形状为3 x 3的最大池化层。\n",
        "#除了使用NiN块以外NiN还有一个设计与AlexNet显著不同：NiN去掉了AlexNet最后的3个全连接层，取而代之的，NiN使用了输出通道数 = 标签类别数的NiN块，\n",
        "#然后使用全局平均池化层对每个通道中所有元素求平均并直接用于分类。这里的全军平均池化层即窗口形状等于输入空间维度形状的平均池化层。NiN的这个设计的好处是可以显著减小模型参数尺寸，\n",
        "#从而缓解过拟合。然而该设计有时会造成获得有效模型的训练时间的增加。\n",
        "\n",
        "net = nn.Sequential()\n",
        "# 前面的卷积层部分，用mlpconv代替了传统的方式；后面的全连接部分用NiN块结合全局平均处理\n",
        "net.add(nin_block(96,kernel_size = 11,strides = 4,padding = 0),\n",
        "       nn.MaxPool2D(pool_size = 3,strides = 2),\n",
        "       nin_block(256,kernel_size=5,strides = 1,padding =2),\n",
        "       nn.MaxPool2D(pool_size = 3,strides = 2),\n",
        "       nin_block(384,kernel_size = 3,strides = 1,padding = 1),\n",
        "       nn.MaxPool2D(pool_size = 3,strides =2),\n",
        "       nn.Dropout(0.5),\n",
        "\n",
        "\t   # 后面部分就是AlexNet的'全连接部分'\n",
        "       # 类别标签，类别数为10，因为使用的手写体\n",
        "       nin_block(10,kernel_size = 3,strides = 1,padding = 1),\n",
        "       # 全局平均池化层将窗口形状自动设置为输入的高和宽\n",
        "       nn.GlobalAvgPool2D(),\n",
        "       # 将四维的输入转化成二维的输出，其形状为（批量大小，10）\n",
        "       nn.Flatten())\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BETmTNq72imZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 使用Fashion - MNIST数据集来训练模型\n",
        "from mxnet.gluon import data as gdata\n",
        "import mxnet as mx\n",
        "from mxnet.gluon import loss as gloss,nn\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "# 需要定义几个函数\n",
        "def load_data_fashion_mnist(batch_size, resize=None, root=os.path.join('~', '.mxnet', 'datasets', 'fashion-mnist')):\n",
        "\t\"\"\"\n",
        "\t用于加载‘fashion-mnist’数据集，并返回一定批次的训练集和测试集\n",
        "\t\"\"\"\n",
        "    root = os.path.expanduser(root)  # 展开用户路径'~'\n",
        "    transformer = []\n",
        "    if resize:\n",
        "        transformer += [gdata.vision.transforms.Resize(resize)]\n",
        "    transformer += [gdata.vision.transforms.ToTensor()]\n",
        "    transformer = gdata.vision.transforms.Compose(transformer)\n",
        "    mnist_train = gdata.vision.FashionMNIST(root=root, train=True)\n",
        "    mnist_test = gdata.vision.FashionMNIST(root=root, train=False)\n",
        "    num_workers = 0 if sys.platform.startswith('win32') else 4\n",
        "    train_iter = gdata.DataLoader(\n",
        "        mnist_train.transform_first(transformer), batch_size, shuffle=True,\n",
        "        num_workers=num_workers)\n",
        "    test_iter = gdata.DataLoader(\n",
        "        mnist_test.transform_first(transformer), batch_size, shuffle=False,\n",
        "        num_workers=num_workers)\n",
        "    return train_iter, test_iter\n",
        "\n",
        "\n",
        "def try_gpu():  \n",
        "\t\"\"\"\n",
        "\t如果有GPU就优先使用，否则使用CPU\n",
        "\t\"\"\"\n",
        "    try:\n",
        "        ctx = mx.gpu()\n",
        "        _ = nd.zeros((1,), ctx=ctx)\n",
        "        print('use gpu')\n",
        "    except mx.base.MXNetError:\n",
        "        ctx = mx.cpu()\n",
        "        print('use cpu')\n",
        "    return ctx\n",
        "\n",
        "def evaluate_accuracy(data_iter, net, ctx):\n",
        "\t\"\"\"\n",
        "\t用于评估模型\n",
        "\t\"\"\"\n",
        "    acc_sum, n = nd.array([0], ctx=ctx), 0\n",
        "    for X, y in data_iter:\n",
        "        # 如果ctx代表GPU及相应的显存，将数据复制到显存上\n",
        "        X, y = X.as_in_context(ctx), y.as_in_context(ctx).astype('float32')\n",
        "        acc_sum += (net(X).argmax(axis=1) == y).sum()\n",
        "        n += y.size\n",
        "    return acc_sum.asscalar() / n\n",
        "\n",
        "\n",
        "def train_ch5(net, train_iter, test_iter, batch_size, trainer, ctx,num_epochs):\n",
        "\t\"\"\"\n",
        "\t定义训练器\n",
        "\t\"\"\"\n",
        "    print('training on', ctx)\n",
        "    loss = gloss.SoftmaxCrossEntropyLoss()\n",
        "    for epoch in range(num_epochs):\n",
        "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
        "        for X, y in train_iter:\n",
        "            X, y = X.as_in_context(ctx), y.as_in_context(ctx)\n",
        "            with autograd.record():\n",
        "                y_hat = net(X)\n",
        "                l = loss(y_hat, y).sum()\n",
        "            l.backward()\n",
        "            trainer.step(batch_size)\n",
        "            y = y.astype('float32')\n",
        "            train_l_sum += l.asscalar()\n",
        "            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().asscalar()\n",
        "            n += y.size\n",
        "        test_acc = evaluate_accuracy(test_iter, net, ctx)\n",
        "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, '\n",
        "              'time %.1f sec'\n",
        "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc,\n",
        "                 time.time() - start))\n",
        "\n",
        "##############################\n",
        "# 开始训练\n",
        "# 定义参数\n",
        "lr,num_epochs,batch_size,ctx  = 0.1,5,128,try_gpu()\n",
        "# 初始化参数，初始化方式：Xavier\n",
        "net.initialize(force_reinit= True,init=init.Xavier())\n",
        "# 初始化训练器\n",
        "trainer = gluon.Trainer(net.collect_params(),'sgd',{'learning_rate':lr})\n",
        "# 加载数据集\n",
        "train_iter,test_iter = load_data_fashion_mnist(batch_size,resize = 224)\n",
        "# 训练模型\n",
        "train_ch5(net,train_iter,test_iter,batch_size,trainer,ctx,num_epochs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpygNmxffs6-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#05.基于keras的GoogLeNet的实现\n",
        "#含并行连接的网络：\n",
        "#Inception块相当于一个有4条线路的子网络。它通过不同窗口形状的卷积层和最大池化层来并行抽取信息，并使用1 x 1 卷积层减少通道数，从而降低模型复杂度。\n",
        "#GoogLeNet将多个设计精细的Inception块和其他层串联起来，其中Inception块的通道数分配之比ImageNet数据集上通过大量的实验得来的。\n",
        "#GoogLeNet和它的后继这们一度是ImageNet上最高效的模型之一：在类似的测试精度下，它们的计算复杂度往往更低。\n",
        "\n",
        "def Conv2d_BN(x, nb_filter,kernel_size, padding='same',strides=(1,1),name=None):\n",
        "    if name is not None:\n",
        "        bn_name = name + '_bn'\n",
        "        conv_name = name + '_conv'\n",
        "    else:\n",
        "        bn_name = None\n",
        "        conv_name = None\n",
        "\n",
        "    x = Conv2D(nb_filter,kernel_size,padding=padding,strides=strides,activation='relu',name=conv_name)(x)\n",
        "    x = BatchNormalization(axis=3,name=bn_name)(x)\n",
        "    return x\n",
        "\n",
        "def Inception(x,nb_filter):\n",
        "    branch1x1 = Conv2d_BN(x,nb_filter,(1,1), padding='same',strides=(1,1),name=None)\n",
        "\n",
        "    branch3x3 = Conv2d_BN(x,nb_filter,(1,1), padding='same',strides=(1,1),name=None)\n",
        "    branch3x3 = Conv2d_BN(branch3x3,nb_filter,(3,3), padding='same',strides=(1,1),name=None)\n",
        "\n",
        "    branch5x5 = Conv2d_BN(x,nb_filter,(1,1), padding='same',strides=(1,1),name=None)\n",
        "    branch5x5 = Conv2d_BN(branch5x5,nb_filter,(1,1), padding='same',strides=(1,1),name=None)\n",
        "\n",
        "    branchpool = MaxPooling2D(pool_size=(3,3),strides=(1,1),padding='same')(x)\n",
        "    branchpool = Conv2d_BN(branchpool,nb_filter,(1,1),padding='same',strides=(1,1),name=None)\n",
        "\n",
        "    x = concatenate([branch1x1,branch3x3,branch5x5,branchpool],axis=3)\n",
        "\n",
        "    return x\n",
        "\n",
        "def GoogLeNet():\n",
        "    inpt = Input(shape=(224,224,3))\n",
        "    #padding = 'same'，填充为(步长-1）/2,还可以用ZeroPadding2D((3,3))\n",
        "    x = Conv2d_BN(inpt,64,(7,7),strides=(2,2),padding='same')\n",
        "    x = MaxPooling2D(pool_size=(3,3),strides=(2,2),padding='same')(x)\n",
        "    x = Conv2d_BN(x,192,(3,3),strides=(1,1),padding='same')\n",
        "    x = MaxPooling2D(pool_size=(3,3),strides=(2,2),padding='same')(x)\n",
        "    x = Inception(x,64)#256\n",
        "    x = Inception(x,120)#480\n",
        "    x = MaxPooling2D(pool_size=(3,3),strides=(2,2),padding='same')(x)\n",
        "    x = Inception(x,128)#512\n",
        "    x = Inception(x,128)\n",
        "    x = Inception(x,128)\n",
        "    x = Inception(x,132)#528\n",
        "    x = Inception(x,208)#832\n",
        "    x = MaxPooling2D(pool_size=(3,3),strides=(2,2),padding='same')(x)\n",
        "    x = Inception(x,208)\n",
        "    x = Inception(x,256)#1024\n",
        "    x = AveragePooling2D(pool_size=(7,7),strides=(7,7),padding='same')(x)\n",
        "    x = Dropout(0.4)(x)\n",
        "    x = Dense(1000,activation='relu')(x)\n",
        "    x = Dense(1000,activation='softmax')(x)\n",
        "    model = Model(inpt,x,name='inception')\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOWnJ_gTgpDs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TIDO\n",
        "\n",
        "#06.基于keras的ResNet的实现\n",
        "\n",
        "#07.基于keras的DenseNet的实现"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}